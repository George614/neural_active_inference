
env_name = CartPole-v1
out_dir = D:/Projects/neural_active_inference/exp/CartPole-v1/qai/pureReward_mse_DQNhyperP_256net_biasInit_start1k_relu_learnSche/
# prior_model_save_path = D:/Projects/neural_active_inference/exp/mcar/prior/prior.agent
#expert_data_path = D:/Datasets/OpenAI_gym/zoo-ppo_MountainCar-v0.npy
eval_model = False #True
use_per_buffer = False
equal_replay_batches = True
plot_rewards = True
batch_size = 64 #256
buffer_size = 100000 #200000
learning_start = 1000
efe_loss = mse
num_episodes = 1000 #5000 #10000 #500 #300 #300 #600
n_trials = 3

# changed EFE starting min at -1 (or 0.01?)
act_fx = relu
efe_act_fx = relu
epsilon_start = 1.0
#epistemic_term = kl #log_diff
normalize_signals = False
epistemic_anneal = False
env_prior = False
instru_term = prior_reward
use_sum_q = False #True
gamma_d = 0.99 #1.0
target_update_ep = 1 #50
#dim_z = 8
dim_o = 4
dim_a = 2
layer_norm = False
optimizer = adam
learning_rate = 0.001 #0.001
learning_rate_decay = -1
l2_reg = 0.0001 #3e-3
clip_type = norm_clip #hard_clip
grad_norm_clip = 10.0 # 10.0
