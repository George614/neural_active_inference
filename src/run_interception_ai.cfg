
env_name = InterceptionEnv
out_dir = D:/Projects/neural_active_inference/exp/interception/qai/separateNN_delay_act_9frame_noEpsGreedy_hdsgtError_DQNhyperP_512net_relu_learnSche_3k/
# prior_model_save_path = D:/Projects/neural_active_inference/exp/interception/prior2/prior.agent
# expert_data_path = D:/Datasets/OpenAI_gym/zoo-dqn_InterceptionEnv-v0.npy
keep_expert_batch = False
eval_model = False #True
use_per_buffer = False
equal_replay_batches = True
plot_rewards = True
record_video = False
batch_size = 256
buffer_size = 10000 #200000
learning_start = 1000
efe_loss = mse  #huber
num_episodes = 3000
n_trials = 10

# changed EFE starting min at -1 (or 0.01?)
act_fx = relu
efe_act_fx = relu
epsilon_start = 1.0
epsilon_final = 0.02
combined_nn = False  # recognition model
epsilon_greedy = False
epistemic_off = False
normalize_signals = False  # normalization for Rti & Rte
epistemic_anneal = False
hindsight_learn = True
env_prior = prior_error  # prior_error or prior_obv or None
instru_term = prior_local # prior_global or prior_local or prior_reward
use_sum_q = False  # update style for Bellman equation
f_speed_idx = 1  # final target speed index
delay_frames = 9  # action delay
rho = 1.0  # epistemic weight factor
gamma_d = 0.99  # TD discount factor
target_update_step = 600
target_update_ep = 3 #50
train_freq = 16  # steps
gradient_steps = 8
net_arch = [512, 512]
dim_o = 4
dim_a = 6
layer_norm = False
optimizer = adam
learning_rate = 3e-4 #0.001
learning_rate_decay = -1 #0.99
l2_reg = -1 #3e-3
clip_type = norm_clip # hard_clip #norm_clip
grad_norm_clip = 10.0 # 10.0
