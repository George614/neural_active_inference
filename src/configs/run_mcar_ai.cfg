
env_name = MountainCar-v0
out_dir = ../exp/MountainCar-v0/qai/DQN/
#prior_model_save_path = ../exp/MountainCar-v0/prior/prior.agent
#expert_data_path = D:/Datasets/OpenAI_gym/zoo-ppo_MountainCar-v0.npy
eval_model = False
use_per_buffer = False
equal_replay_batches = True
plot_rewards = True
batch_size = 128
buffer_size = 10000
learning_start = 1000
efe_loss = mse
num_episodes = 2000
n_trials = 3

# changed EFE starting min at -1 (or 0.01?)
act_fx = lrelu
efe_act_fx = lrelu
epsilon_greedy = True
epsilon_start = 1.0
epsilon_final = 0.07
epistemic_off = True
normalize_signals = False
epistemic_anneal = False
#env_prior = prior_error
instru_term = reward # prior_local # prior_global
use_sum_q = False
rho = 1.0
gamma_d = 0.98
target_update_ep = 2
target_update_step = 600
train_freq = 16
gradient_steps = 8
dim_o = 2
dim_a = 3
net_arch = [256, 256]
layer_norm = False
optimizer = adam
learning_rate = 4e-3
learning_rate_decay = -1
l2_reg = -1
clip_type = norm_clip
grad_norm_clip = 10.0
